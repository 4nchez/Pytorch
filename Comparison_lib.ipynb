{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 속도 비교 Numpy vs tensorflow vs Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]]\n",
      "[[ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]]\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "0:00:00.001995\n"
     ]
    }
   ],
   "source": [
    "# 연산에 필요한 numpy, 시간을 측정하기 위해 datetime을 불러옵니다.\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# 랜덤하게 3x4 형태의 변수 x,y,z를 설정해줍니다.\n",
    "np.random.seed(0)\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x = np.random.randn(N,D)\n",
    "y = np.random.randn(N,D)\n",
    "z = np.random.randn(N,D)\n",
    "\n",
    "# x,y,z를 이용해 x*y+z를 계산해줍니다.\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "# 기울기(gradient)가 1이라고 가정하고 역전파를 해줍니다. 역전파에 대한 내용은 4장에서 자세히 다룹니다.\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_y = grad_a * y\n",
    "grad_x = grad_a * x\n",
    "\n",
    "# 각각의 기울기가 몇인지 걸린 시간은 얼마인지 확인해봅니다.\n",
    "print(grad_x)\n",
    "print(grad_y)\n",
    "print(grad_z)\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[[-1.6138978  -0.21274029 -0.89546657  0.3869025 ]\n",
      " [-0.51080513 -1.1806322  -0.02818223  0.42833188]\n",
      " [ 0.06651722  0.3024719  -0.6343221  -0.36274117]]\n",
      "[[ 1.2302907   1.2023798  -0.3873268  -0.30230275]\n",
      " [-1.048553   -1.420018   -1.7062702   1.9507754 ]\n",
      " [-0.5096522  -0.4380743  -1.2527953   0.7774904 ]]\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "0:00:07.218554\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# 이번에는 텐서플로 프레임워크를 이용해 같은 연산을 해보도록 하겠습니다.\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# 텐서플로는 연산 그래프를 먼저 정의하고 추후에 여기에 값을 전달하는 방식입니다. 여기서는 비어있는 그래프만 정의해줍니다.\n",
    "# Define Graph on GPU\n",
    "x = tf.placeholder(tf.float32)     # 비어있는 노드인 placeholder를 정의하고 여기에 들어가는 데이터타입을 명시 해놓습니다.\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "a = x * y                          # 연산 과정 또한 정의해줍니다.\n",
    "b = a + z\n",
    "c = tf.reduce_sum(b)\n",
    "    \n",
    "grad_x, grad_y, grad_z = tf.gradients(c,[x,y,z])  # c에 대한 x,y,z의 기울기(gradient)를 구하고 이를 각각 grad_x, grad_y, grad_z에 저장하도록 지정해놓습니다.\n",
    "\n",
    "# 실제적인 계산이 이루어지는 부분. 텐서플로에서는 이를 세션이라고 합니다.\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N,D),     # 여기서 실제 값들이 생성됩니다.\n",
    "        y: np.random.randn(N,D),\n",
    "        z: np.random.randn(N,D)           \n",
    "    }\n",
    "    out = sess.run([c,grad_x,grad_y,grad_z],feed_dict = values)  # 세션에서 실제로 값을 계산하는 부분입니다. feed_dict를 통해서 값들을 전달합니다.\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "# 값들을 확인하고 걸린 시간을 측정합니다.\n",
    "print(grad_x_val)\n",
    "print(grad_y_val)\n",
    "print(grad_z_val)\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1422,  0.4268, -0.7289,  0.6269],\n",
      "        [-0.3137,  0.6453, -0.8428, -1.9463],\n",
      "        [ 1.1105,  0.3116, -0.0405, -0.9286]], device='cuda:0')\n",
      "tensor([[ 0.0115,  0.1044, -0.0936,  0.1393],\n",
      "        [-0.5642, -1.0135, -1.7651,  1.1080],\n",
      "        [-1.7543, -0.5027,  1.1579,  1.7286]], device='cuda:0')\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='cuda:0')\n",
      "0:00:01.949431\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "y = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "z = Variable(torch.randn(N,D).cuda(),requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "# c.backward(gradient=torch.cuda.FloatTensor([1.0]))\n",
    "c.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(datetime.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
